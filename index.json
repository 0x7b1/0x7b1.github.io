[{"content":"This is projects aimed to systematically compare relevant RDF relational schemas, i.e., Single Statement Table, Property Tables or Vertically-Partitioned Tables queried using Apache Spark.\nIt is evaluated the performance Spark SQL querying engine for processing SPARQL queries using three different storage back-ends, namely, Postgres SQL, Hive, and HDFS. For the latter one, the experiment compares four different data formats (CSV, ORC, Avro, and Parquet). It uses a representative query workloads from the SP2Bench benchmark scenario.\nThe results show interesting insights about the impact of the relational encoding scheme, storage backends and storage formats on the performance of the query execution process. You can visit more in the project website.\nLinks  Source code Benchmarking Spark-SQL under Alliterative RDF Relational Storage Backends Towards making sense of Spark-SQL performance for processing vast distributed RDF datasets  ","permalink":"https://0x7b1.github.io/projects/spark-rdf/","summary":"Systematic comparison of RDF relational schemas using Apache Spark.","title":"SparkSQL RDF Benchmarking"},{"content":"Traditional in-memory data structures do not optimally utilize on-CPU caches. Hash tables for indexes are fast but only support point queries. ART is very space efficient and solves the problem of excessive worst-case space consumption. ART, an adaptive radix tree (trie) for efficient indexing in main memory. It maintains the data in sorted order, which enables additional operations like range scan and prefix lookup.\nIntroduction For OLTP workloads, the resulting execution plans are often sequences of index operations. Therefore, index efficiency is the decisive performance factor T-tree was proposed as an in-memory indexing structure The k-ary search tree and the Fast Architecture Sensitive Tree (FAST) use data level parallelism to perform multiple comparisons simultaneously with Singe Instruction Multiple Data (SIMD) instructions. However, they cannot support incremental updates Hash tables are less commonly used as database indexes because scatter the keys randomly, and therefore only support point queries. Also do not handle growth gracefully, but require expensive reorganization upon overflow Therefore, current systems face the unfortunate trade-off between fast hash tables that only allow point queries and fully-featured, but relatively slow, search trees. A third class of data structures, known as trie, radix tree, prefix tree, and digital search tree While most radix trees require to trade off tree height versus space efficiency by setting a globally valid fanout parameter, ART adapts the representation of every individual node A useful property of radix trees is that the order of the keys is not random as in hash tables; rather, the keys are ordered bitwise lexicographically operations that require the data to be ordered (e.g., range scan, prefix lookup, top-k, minimum, and maximum).\nRelated Work  In disk-based database systems, the B+-tree is ubiquitous T-trees, like all binary search trees, suffer from poor cache behavior and are therefore often slower than B+-trees on modern hardware. Workaround: a cache conscious B+-tree variant, the CSB+-tree Modern CPUs allow to perform multiple comparisons with a single SIMD instruction kary search → FAST: are pointer-free data structures which store all keys in a single array and use offset calculations to traverse the tree. While this representation is efficient and saves space, it also implies that no online updates are possible. GPUs as dedicated indexing hardware is not yet practical because memory capacities of GPUs are limited, communications cost with main memory is high The two earliest variants use lists and arrays as internal node representations radix tree over trie because it underscores the similarity to the radix sort algorithm and emphasizes that arbitrary data can be indexed instead of only character strings The idea of dynamically changing the internal node representation is used by KISS-Tree, Generalized Prefix Tree, Judy array. they work as a general-purpose indexing structure binary-comparable (“normalized”) keys: simplifying and speeding up key comparisons  The Adaptive Radix Tree Motivate the use of adaptive nodes by showing that the space consumption of conventional radix trees can be excessive.\n Properties of a Radix tree  The height depends on the lenght of the key, not on the number of keys Require no balancing ops Keys stored in lexicographic order The path to a leaf node represents the key of that leaf    Radix mantains two type of nodes: Inner nodes, which map partial keys to other nodes, and leaf nodes, which store the values corresponding to the keys. The most efficient representation of an inner node is as an array of 2^s pointers. The parameter s (span), is critical for performance, it determines the height of the tree for a given key length. A radix tree storing k bit keys has k/s levels of inner nodes.. comparison-based search trees: it is illustrative to compare the height of radix trees with the number of comparisons in perfectly balanced search trees. a radix tree node can rule out more values if s\u0026gt;1. BST search takes O(klogn) whereas radix takes O(k).\nAdaptive Nodes It is desirable to have a large span. Space usage becomes excessive when using array of pointers and childs are null. As the span increases, the tree height decreases but the space also increases. Only some values of s offer a balanced tradeoff between space and time. ART uses less space and has smaller height than normal radix. The idea is to adapively use different node sizes with same large span w. diff fanout.\nStructure of Inner Nodes   Data structures for inner nodes    Inner nodes map partial keys to child pointers Four data structures with different capacities  Node4: up to 4 child pointers and uses an array of length 4 for keys and another array of the same length for pointers Node16: for storing between 5 and 16 child pointers. A key can be found efficiently with binary search or, on modern hardware, with parallel comparisons using SIMD instructions. Node48: nodes with more than 16 pointers do not store the keys explicitly. this array stores indexes into a second array which contains up to 48 pointers Node256: the next node can be found very efficiently using a single lookup of the key byte in that array. is also very space efficient because only pointers need to be stored.   At the front of each inner node, a header of constant size (e.g., 16 bytes) stores the node type, the number of children, and the compressed path The child pointers can be scanned in sorted order, which allows to implement range scans Bytes are directly addressable which avoids bit shifting and masking operations Instead of using a list of key/value pairs, it splits the list into one key part and one pointer part. Refer to Figure 5 of the paper.  Operations Search  The tree is traversed by using successive bytes of the kay array until a leaf node or a null pointer is encountered Depending on the node type the approapriate search algorithm is executed. For example Node16 uses SSE implementation with SIMD which allows to compare 16 keys stored with one instruction in parallel.  Insert  replace substitutes a node in the tree by another node addChild appends a new child to an inner node checkPrefix compares the compressed path of a node with the key loadKey retrieves the key of a leaf from the database  Bulk loading  Using the first byte of each key the key/value pairs are radix partitioned into 256 partitions Before returning that inner node, its children are created by recursively applying the bulk loading procedure for each partition using the next byte of each key  Delete  deletion is symmetrical to insertion: leaf is removed from an inner node and then shrunk if necessary. If the node has only 1 child, it’s replaced by its child and comprsd.  Conclusions ART is a fast and space-efficient indexing structure for main-memory database system. A high fanout, path compression, and lazy expansion reduce the tree height, and therefore lead to excellent performance. ART is compared with other state-of-the-art main-memory data structures. Results show that ART is much faster than a red-black tree, a Cache Sensitive B+-Tree, and GPT, another radix tree proposal. ART is a superior alternative to conventional index structures for transactional workloads.\nLinks  The Adaptive Radix Tree: ARTful Indexing for Main-Memory Databases  ","permalink":"https://0x7b1.github.io/posts/papers/2020-12-10-artful/","summary":"By this and that","title":"Paper review: ARTful Indexing for Main-Memory Databases"},{"content":"This paper introduces HOT, a trie index structure that enables efficient search and insert operations using modern processor instructions by being space efficient and linear scalable. The basic idea is to dynamically vary the number of bits considered at each node and enable high fanout and low tree height. The major difference between this and radix tree variants is its variable size for each node. HOT outperforms state of the art index structures in terms of search performance and memory footprint. These prevalent differences makes it appealing to be used as a general purpose index structure for main-memory database systems or string intensive applications.\nIntroduction Memory database systems depend on fast index structures. For this reason, it is required to be space efficient and responsive. One advantage of using tries, compared to hash tables, is its order-preserving property. ART achieves high fanout and high performance on integers but its average fanout is lower when indexing strings. This is caused by sparse key distributions prevalence in string keys. HOT is a space-optimized and performant index structure that dynamically evolves and efficiently support operations. It uses an adaptive number of bits (also known as span) per nodes depending on the data distribution to achieve a high average fanout. In this way, the sparsity problem and space consumption are reduced. HOT also provides a careful design to leverage the power of modern CPU instructions by making intensive use of SIMD parallel operations. For the benchmark, HOT is compared against state-of-the-art index structures showing outstanding results.\nBackground Trie is a tree structure where all descendants of a node share a common prefix. The major drawbacks of tries are the large heights. Patricia trees omit nodes with only one child, the result is a full binary tree. This reduces the height but the fanout remains small, and therefore still yields large tree heights. In order to reduce the height, one technique is to increment the size of the span s and make each node to store (2^s) pointers. Large spans are space consuming but they can be reduced by dynamically adapting the node structure, like ART does by using a compact representation of the store instead of a fixed array of 256 pointers. Having a span of 8 bits results in sparsity of distributed keys in many nodes with very low fanout. Using adaptive nodes reduces the memory consumption in sparsely distributed data, but does not address the problem of a balanced fanout and height of the tree. The idea is to reduce the height of the tree and increase the average node fanout. HOT builds upon these previous ideas to propose a binary Patricia trie with compound nodes by having a maximum node fanout and an optimized resulting height. Previous work has proposed hybrid data structures but they perform concisely only on specific workloads.\n  Figure 1. Different implementations of a Trie, and HOT   Implementation As mentioned, the most important optimization is to increase the span of each compound node. We can observe in the Figure 1 how the span and fanout varies across different impementations of a trie. The problem lies on sparsely-distributed keys that usually string values cause when using a trie with fixed size of span. HOT proposes to set the size of the span per node depending on the data distribution by targeting a maximum fanout k. In HOT, every compound node represents a binary Patricia trie with a fanout up to k, with n keys and n − 1 inner nodes. This means at most k − 1 binary inner nodes. Given a k of fanout the idea is to minimize the overall tree height associated with h(n), which yields the maximum height of its compound child nodes. The minimization of the height of the tree is analogous to partitioning a full binary tree. This idea preserves the height optimization while new data is inserted. A (compound) node contains up to k − 1 patricia trie nodes and k leaf entries.\nOrganization   Figure 2. Binary encoding using a sequential layout   HOT does not organize nodes in a pointer-based trie structure, instead uses a compact representation that allows it to be space-efficient and fast, as it is shown if the Figure 2. The idea is to linearize a trie bit string in order to allow parallel searching using SIMD instructions. The first dimension of the node is the size of the partial keys, and the second is the representation of the bit positions. These representations are meant to be optimized by modern processor instruction sets. Each node layout consists of header, bit positions, partial keys, and values. The lookup operation traverses the tree until a leaf node containing a tuple identifier is encountered. The insertion operation uses a sparse partial key for inner nodes. First, a search operation is issued to check its existence, and then an intricate operation to flip bites using SIMD is done in order to insert new bit positions. This operation is further improved by encoding the least-significant bits and overlapping operations.\nScalability Scalability is important for an index structure, for this a synchronization protocol is used in order to provide efficient concurrent index access. Traditional locking tech-iques don’t scale. Lock-free index structures or write-only minimal locks. For synchronizing HOT, the protocol takes care of the insert and delete operations. One important aspect of the synchronization is making the nodes obsolete instead of claiming directly to memory, this allows concurrent writes and reads with no locking.\nEvaluation For the benchmarking, there are 5 workloads that test operations according to different proportions by using four different datasets consisting of string data. HOT is compared against state of the art index structures: ART, MassTree, BT. The benchmark evaluates performance, memory consumption scalability and tree height.\n Performance: HOT is better on many of the workload scenarios. It is also consistent across different dataset shapes, which makes it appealing for a general purpose index structure. Memory consumption: The evaluation measures the space required to store nodes, and key identifiers. HOT is the most space-efficient data structure, and also the one that has less memory footprint. Scalability: The workload executes insert and lookup operations to measure the throughput of operations. HOT features a linear scalability mainly due to the employed synchronization protocol. Tree height: An analysis of the depth distribution for all data sets shows that HOT is able to reduce the mean depth significantly in contrast with the other trie structures.  Conclusion HOT is a novel index data structure that improves traditional tries by tackling the problem of speed and sparsity. This enables instant lookup and fast insert operations, which is one of the requirements of Database Systems.\nLinks  HOT: A Height Optimized Trie Index for Main-Memory Database Systems  ","permalink":"https://0x7b1.github.io/posts/papers/2020-11-21-hot/","summary":"HOT is a efficient radix tree that uses a general purpose index structure for main-memory database systems or string intensive applications.","title":"Paper review: HOT Radix Tree"},{"content":"This project is based on the Lab assignments used in MIT\u0026rsquo;s distributed systems course (6.284). It is divided in 3 main parts and is expected to be implemented using the Go programming language.\nPart I: Distributed MapReduce Go to the Lab\u0026rsquo;s description\nThe most important parts of the Mapreduce model are:\n The Figure 1 of the paper. The Map and Reduce functions  map (k1, v1) -\u0026gt; list(k2, v2) reduce (k2, list(v2)) -\u0026gt; list(v2) In the MapReduce model there are two types of nodes: master and worker. The master is responsible for dividing the task into M tasks. As long as there is idle worker, it will execute the task map, and R results will be generated and stored on disk. The worker who executes reduce later will get the corresponding map result from other workers through RPC according to his number (0 to R-1). The reducer will get a number (k2, list(v2)) of key-value pairs, and then pass this to the reduce-function (user-defined), and the result will be stored and it will be done. The next step is to merge the results of reduce.    A classic example is the word-counter.\n Having a bunch of files that need to count characters. First, step is to assign the files to each map. It will count the characters and output key-value pairs:  Hello, my name is x, your name? Hello, your name is nice  The above two sentences, through map, it\u0026rsquo;s assigned to two map operations:  1: \u0026lt;hello,1\u0026gt; \u0026lt;my,1\u0026gt; \u0026lt;name,2\u0026gt; \u0026lt;is,1\u0026gt; \u0026lt;x,1\u0026gt; \u0026lt;your,1\u0026gt; 2: \u0026lt;hello,1\u0026gt; \u0026lt;name,1\u0026gt; \u0026lt;is,1\u0026gt; \u0026lt;your,1\u0026gt; \u0026lt;nice,1\u0026gt; |------------reduce 1--------------|----------- ---reduce 2------------------|  Before throwing the output key-value pairs into the corresponding reduce, we need to obtain its set:  \u0026lt;hello,\u0026lt;1,1\u0026gt;\u0026gt; \u0026lt;my,1\u0026gt; \u0026lt;name,\u0026lt;2,1\u0026gt;\u0026gt; ===\u0026gt; reduce 1 \u0026lt;is,\u0026lt;1,1\u0026gt;\u0026gt; \u0026lt;x,1\u0026gt; \u0026lt;your,\u0026lt;1,1 \u0026gt;\u0026gt; \u0026lt;nice,1\u0026gt; ===\u0026gt; reduce 2  Finally, reduce will count its value:  reduce 1 ===\u0026gt; \u0026lt;hello,2\u0026gt; \u0026lt;my,1\u0026gt; \u0026lt;name,3\u0026gt; reduce 2 ===\u0026gt; \u0026lt;is,2\u0026gt; \u0026lt;x,1\u0026gt; \u0026lt;your,2\u0026gt; \u0026lt;nice,1\u0026gt; During the whole stage, we rely on a master and several workers to conduct orderly operations First, we start a master and three workers; the master will tell the workers that it is now in the map phase, you execute file 1, you execute file 2 It is necessary for the worker to report to the master that he has done it. Because this will tell the master whether it should proceed to the next reduce phase. If all files are processed, the master will adjust the status to reduce: But when we actually come to the experiment, we need to consider some issues.\n A master, which members need to be included in the worker How to represent tasks and how to distribute tasks First conceive the whole experiment. We\u0026rsquo;d better not be mechanically process-oriented, that is, read the file list -\u0026gt; for loop, traverse the file list -\u0026gt; call (rpc) n workers remotely at the same time, and pass in the file name in the master. , Let it execute Map; master maintains a key-value pair to know whether all files have completed the Map phase; then perform Reduce. Knowing the entire process framework, we return to the first question, what kind of member variables should be designed. Map task, we need it to process a separate file, so there is a string type file, a task must have its own ID, and we must know which Worker we work for (workerid), and we also need to know this task Is it done? (status); the same is true for Reduce. Master Structure, the master needs to maintain two task queues:   A set of pointer lists for saving tasks locally, MapTasks \u0026amp; ReduceTasks; A set of channels MapTasksChan \u0026amp; ReduceTasksChan for asynchronous communication, Worker will request tasks remotely through rpc; The number of NReduce, given by initialization, 10; NMap represents the number of files, calculated when the master is initialized; Represents the number of completed tasks NCompleteXXX;  Process, after the master initializes some of its member variables, it will directly start to generate Map tasks. This process will write the Map tasks into the queue, and at the same time wait for the worker\u0026rsquo;s RPC call to take away the secondary tasks. It is worth noting that the allocation task in the above figure uses select, which will respond selectively according to the arrival of the channel, that is, return to the Map task when the Map task arrives, and return to the Reduce task when the Reduce task arrives first; MonitorMapTask() , This function will monitor whether the Map task is completed, which is very important because it will hang up a worker later in the test case.  Part II: Raft, a replicated state machine protocol In this lab the goal is to build a fault-tolerant key/value storage system, which is divided into three parts:\n Implement Raft, a replicated state machine protocol Build a key/value service on Raft Share among multiple replicated state machines Service for higher performance  Nodes have three states, and the initial state of all nodes is Follower.\n Follower Candidate Leader  Basic Structures At first, you may not know where to start. First, write a part of the framework according to the hints given in the course document.\n Add any state you need to the Raft struct in raft.go. You\u0026rsquo;ll also need to define a struct to hold information about each log entry. Your code should follow Figure 2 in the paper as closely as possible.\n type Log struct { Command interface{} Term int32 } // // A Go object implementing a single Raft peer. // type Raft struct { mu sync.Mutex // Lock to protect shared access to this peer\u0026#39;s state  peers []*labrpc.ClientEnd // RPC end points of all peers  persister *Persister // Object to hold this peer\u0026#39;s persisted state  me int // this peer\u0026#39;s index into peers[]  // counts int  // Your data here (2A, 2B, 2C).  // Look at the paper\u0026#39;s Figure 2 for a description of what  // state a Raft server must maintain.  // Persistent state on all servers  logs []Log votedFor int currentTerm int32 // State  state int32 // Volatile state on all server  commitIndex int lastApplied int VoteGrantedCount int // Volatile state on leaders  nextIndex[] int matchIndex[] int // timer  electionTimer *time.Timer voteCh chan struct{} }  Fill in the RequestVoteArgs and RequestVoteReply structs.\n type RequestVoteArgs struct { // Your data here (2A, 2B).  Term int CandidateId int LastLogIndex int LastLogTerm int } // field names must start with capital letters! type RequestVoteReply struct { // Your data here (2A).  Term int VoteGranted bool }  Modify Make() to create a background goroutine that will kick off leader election periodically by sending out RequestVote RPCs when it hasn\u0026rsquo;t heard from another peer for a while.\n The preparation of Make is very important, it is actually the initialization process of Raft. Finally, the loop of the state machine is started.\nfunc Make(peers []*labrpc.ClientEnd, me int, persister *Persister, applyCh chan ApplyMsg) *Raft { rf := \u0026amp;Raft{} rf.peers = peers rf.persister = persister rf.me = me // Your initialization code here (2A, 2B, 2C).  rf.currentTerm = 0; rf.votedFor = -1; rf.VoteGrantedCount = 0; rf.logs = make([]Log, 0) rf.commitIndex = -1 rf.lastApplied = -1 rf.state = FOLLOWER rf.nextIndex = make([]int, len(rf.peers)) rf.matchIndex = make([]int, len(rf.peers)) // initialize from state persisted before a crash  rf.readPersist(persister.ReadRaftState()) go rf.raftLoop() return rf }  This way a peer will learn who is the leader, if there is already a leader, or become the leader itself. Implement the RequestVote() RPC handler so that servers will vote for one another.\n Implementation func (rf *Raft) RequestVote(args *RequestVoteArgs, reply *RequestVoteReply), as explained in the RequestVote RPC part of the paper, Recever implememtation, is transformed into the following code:\nfunc (rf *Raft) RequestVote(args *RequestVoteArgs, reply *RequestVoteReply) { // Your code here (2A, 2B).  rf.mu.Lock() defer rf.mu.Unlock() if args.Term \u0026lt; rf.currentTerm { reply.VoteGranted = false } else if args.Term == rf.currentTerm { if (rf.votedFor == -1 || rf.votedFor == args.CandidateId) \u0026amp;\u0026amp; args.LastLogIndex \u0026gt;= len(rf.logs){ // Only when the node does not vote for the candidate or only for the current candidate, the vote is successful. Ensure that a node votes twice  reply.VoteGranted = true rf.votedFor = args.CandidateId; fmt.Printf(\u0026#34;Server %d voted for Server %d\\n\u0026#34;, rf.me, args.CandidateId) // rf.resetTimer()  } else { reply.VoteGranted = false } } else { // currentTerm is smaller than the candidate  rf.switchTo(FOLLOWER) rf.currentTerm = args.Term rf.votedFor = args.CandidateId reply.VoteGranted = true } reply.Term = rf.currentTerm if reply.VoteGranted == true { go func() { rf.voteCh \u0026lt;- struct{}{} }() } }  To implement heartbeats, define an AppendEntries RPC struct (though you may not need all the arguments yet), and have the leader send them out periodically.\n type AppendEntryArgs struct { term int // leader\u0026#39;s term  leaderId int // so follower can redirect clients  prevLogIndex int // index of log entry immediately preceding new ones  prevLogTerm int // term of prevLogIndex entry  entries []Log // (empty for heartbeat; may send more than one for efficiency)  leaderCommit int // leader\u0026#39;s commitIndex } type AppendEntryReply struct { term int // currentTerm, for leader to update itself  success bool // true if follwer contained entry matching prevLogTerm and prevLogIndex } func (rf *Raft)sendAppendEntries(server int , args *AppendEntryArgs, reply *AppendEntryReply) { ok := rf.peers[server].Call(\u0026#34;Raft.AppendEntries\u0026#34;, args, reply) return ok }  Make sure the election timeouts in different peers don\u0026rsquo;t always fire at the same time, or else all peers will vote only for themselves and no one will become the leader. In RequestVotefunction, we added a judgment: only if this node did not vote for any candidate or ballot to the current candidates, this is the ticket to success. Ensure that a node votes twice\n if (rf.votedFor == -1 || rf.voteFor == args.CandidateId) \u0026amp;\u0026amp; args.LastLogIndex \u0026gt;= len(rf.logs){ reply.VoteGranted = true rf.votedFor = args.CandidateId; } Function implementation  First realize the atomic operations of state acquisition, GetTerm and state judgment. The conffollowing functions are also used when checking in (if you run the test error, you need to check whether the function is implemented).  // atomic operations func (rf *Raft) getTerm() int32 { return atomic.LoadInt32(\u0026amp;rf.currentTerm) } func (rf *Raft) isState(state int32) bool { return atomic.LoadInt32(\u0026amp;rf.state) == state } // return currentTerm and whether this server // believes it is the leader. func (rf *Raft) GetState() (int, bool) { var term int var isleader bool // Your code here (2A).  term = int(rf.getTerm()) isleader = rf.isState(LEADER) return term, isleader } Realize the reply of request to vote, handle request and handle request to vote.\n func (rf *Raft) broadcastVoteReq() func (rf *Raft) RequestVote(args *RequestVoteArgs, reply *RequestVoteReply) func (rf *Raft) sendRequestVote(server int, args *RequestVoteArgs, reply *RequestVoteReply) bool  Realize request to add Log item, handle append request and handle request append reply. The implementation of PartA only stays at using AppendEntries to maintain the existence of the Leader, and does not actually change the Log.\n func (rf *Raft) sendAppendEntries(server int , args *AppendEntryArgs, reply *AppendEntryReply) bool func (rf *Raft) broadcastAppendEntries() func (rf *Raft) AppendEntries(args *AppendEntryArgs, reply *AppendEntryReply)  To achieve randomness, Golang\u0026rsquo;s timer + rand is enough. The following two functions implement random time generation and Timer reset.\nfunc randElectionDuration() time.Duration { r := rand.New(rand.NewSource(time.Now().UnixNano())) return time.Millisecond * time.Duration(r.Int63n(ELEC_TIME_MAX-ELEC_TIME_MIN) + ELEC_TIME_MIN) } func (rf *Raft) resetTimer() { newTimeout := randElectionDuration() rf.electionTimer.Reset(newTimeout) } The last and most important function is the switching logic between Raft\u0026rsquo;s states, which is Raft\u0026rsquo;s main loop.\nfunc (rf *Raft) raftLoop() { rf.electionTimer = time.NewTimer(randElectionDuration()) for { switch atomic.LoadInt32(\u0026amp;rf.state) { case FOLLOWER: select { case \u0026lt;-rf.voteCh: rf.resetTimer() case \u0026lt;- rf.electionTimer.C: rf.mu.Lock() rf.switchTo(CANDIDATE) rf.startElection() rf.mu.Unlock() } case CANDIDATE: rf.mu.Lock() select { case \u0026lt;-rf.electionTimer.C: rf.resetTimer() // election time out ， what we should do? do it again  rf.startElection() default: // check if it has collected enough vote  if rf.VoteGrantedCount \u0026gt; len(rf.peers)/2 { rf.switchTo(LEADER) } } rf.mu.Unlock() case LEADER: rf.broadcastAppendEntries() time.Sleep(HEART_BEAT) } } } Part III: Fault-tolerant key/value storage service This experiment is the third experiment of the course. It completes a key-value system based on the raft protocol.\nThe service must support 3 operations\nPut ( key , value ) : change the value of key Append ( key , arg ) : add value to the value of key Get ( key ) : return value  When packet loss is not performed and the case of realizing servers fail, it is necessary to provide the client sequential consistency of api, call Put, Append a Get3 and api, performed in the same order in all the server, and having at-most-once the A suggested plan for semantics is: first complete server.gothe Opstructure in progress , and then complete server.gothe PutAppend()sum Get()operation. In operation, it should be called first Start(), and when the log commits, reply to the client\n  After the call Start(), kvraft servers will wait for the raft log to reach an agreement. By applyChobtaining consistent commands, we need to consider how to arrange the code so that it can be read continuously applyCh, and other commands can also be executed. We need to deal with the case: the leader is called Start(), but leadership is lost before log commit. In this case, the code should resend the request to the new leader. One way is that the server needs to detect that it is no longer the leader, and returns an unused request on the index by checking the same start. The other way is by calling GetState(), but if there is a network partition, it may not know that it is no longer the leader. In this case, both the client and server are in the network partition, so they can wait indefinitely until the network is restored A kvraft server should not complete the Get()operation if it cannot get the majority, because it may not get the latest data.  Need to request a number for each client To ensure that the memory is released quickly, so the next request can be brought with the next request\nCurrent problem: leader changes frequently:\nfunc (ck * Clerk) Get(key string) string { args := GetArgs { Key : key } for { for _ , c := range ck . servers { time . Sleep ( time . Millisecond * 1000) reply := GetReply { } ok := c . Call (\u0026#34;RaftKV.Get\u0026#34; , \u0026amp;args , \u0026amp;reply) if ok \u0026amp;\u0026amp; !reply.WrongLeader { return reply.Value } } } return \u0026#34;\u0026#34; } If there is no sleep here, it is equivalent to that the client is constantly in START. The problem is that the server is constantly processing the START command, causing the normal heartbeat to be unable to complete, and frequent changes to the leader occur. The problem is very serious. Seriously, what should I do? Later, optimization was made. For read operations, chan is not used, which is not a problem.\nindex := -1 Term := -1 isLeader := true if rf . state != StateLeader { isLeader = false return index , term , isLeader } Give each client a number, and then each request grows sequentially\nselect { case op := \u0026lt;-ch: commited := op == entry kv.logger.Debug(\u0026#34;index:%d commited:%v\u0026#34;, index, commited) return commited // The timeout here is actually very easy to understand, because it was the leader at the beginning, but before the log got the commit, the leadership was lost. At this time  // If there is no timeout mechanism, it will block forever  // Or because the leader at this time is the leader in a partition, it can only be blocked forever  // So also need timeout  case \u0026lt;-time.After(AppendTimeOut): //kv.logger.Info(\u0026#34;index:%d %s timeout after %v\u0026#34;, index, entry.Type, AppendTimeOut)  return false } Links  Source code  ","permalink":"https://0x7b1.github.io/projects/mit-6.824-project/","summary":"This course project implements the Raft Consensus algoritm, Map Reduce, and a Fault-tolerant key/value storage service.","title":"MIT's 6.824 Distributed System Course"},{"content":"This project implements a sandbox simulation of liquid physics using cellular automata. The simulation will invite users to explore their creativity and think about ways to create different scenarios and see the reactions with objects by being able to make modifications in real time. To achieve the dynamics of fluid, this project will implement different sets of cellular automata rules and formulas needed for the physical constraints. This program will be implemented for the desktop in order to get the most out of the computer resources using a low level language. (compared to the web or mobile).\nBackground and Motivation There exists a kind of simulation called “Falling-sand” which are games that provide a canvas to paint different elements and see how the particles interact between them in different ways. These games have been around for a while allowing you to interact with different mechanics and using creative elements. Some of them achieve really complex element behaviors and others are actually implementations of academic research. To mention just a few we have The Powder Toy, dan-ball.jp’s Powder Game as the oldest ones, and Sandspiel and Noita which are very recent games implemented with new approaches/techniques.\n  The Powder Toy   What they have in common is the use of cellular automata to make the elements interact with each other which are based on rules that have an evolving nature. Simulation of fluid physics is hard to implement and there are multiple approaches for doing it. Although the use of Navier–Stokes equations can achieve good results, cellular automata provides an interesting, appealing, and cheaper way to get similar results. The goal of the project is to provide the ability to interact with the sandbox in real time with the provided tools (that will be described above). Furthermore, this simulation is a low-res 2D grid based style. To do so, many things have to be taken into consideration such as the implementation of the algorithm, the environment and development tooling, and a good sense of mathematics. These are the main motivations for doing this project.\nBeing described what the project is about, now the description of features are going to be presented.\n Spawn liquid: This is the main feature because it will allow the user to create chunks of liquid from the mouse position within the canvas. This portion of liquid will immediately start to interact with the other elements wherever it is being placed. Create solid walls: This feature will allow liquid to collide with solid objects that can be painted with the mouse. The option for erasing wall pixels is also considered. Rotate canvas: This is a personal challenging feature. With this the user has the ability to grab the canvas and rotate (around the mass center of the canvas) in XY direction and see how the liquid reacts with the solid objects and gravity.  Milesone 1 Brush up the practice on Rust Initially this language was decided to be used because it provides many benefits related to performance. However its relatively steep learning curve required it to dedicate a decent amount of time trying to learn the features and quirks of the language to start doing the main work. As my experience goes, I had some experience with Rust a while ago but I didn’t dig properly into its fascinating and well-known features that make it unique. That was my goal for this task, to try to re learn the language and practice to gain more experience by using the language enough with the concepts that would make the simulation be fast and secure. The main resources utilized now and throughout the progress of this project are:\nPlay around with a API for graphics The road to pick the desired graphics library involved to try and implement fair enough code and decide which one to choose. The starting point was this resource which gives a great insight on which kind of library to use. By popularity, the most used starred in Rust is named gfx which leverages the power of Vulkan in the Desktop. However, as the complexity of using low level APIs is out of scope of this project, these kinds of libraries were discarded (as well as those one who are wrappers of OpenGL, DirectX). Later I found two promising 2D graphics libraries which provide good constructs, methods and structs to work with, those are graphics and minifb. These are not quite popular but suits the needs of this project. After experimenting with both, the picked one was the latter because the handling of input events, buffer allocation and pixel rendering were more convenient (also because it provides a nice math library for doing color and vec operations).\n  Trivial implementation of noise pixel rendering using minifb   Setup simulation structures, canvas and code After trying to visualize the whole picture of structures needed for the simulation, the tentative ones to be used (for now) are: Cell and World. Cell holds the properties of a single pixel in the grid; World holds the grid which is a matrix of Cells. Each one has its own methods to handle and update the data. Cell contains methods for updating its internal state (coloring), and World does the respective manipulations of cell evolution (creating a new generation) of cells. The purpose of this task was also to gain familiarity with Rust features such as traits, structs, I/O streams and boxes (memory heap allocations).\nImplement a basic CA To tackle this task with the purpose of revealing the power of the graphics library in conjunction with the features of this programming language, a basic Cellular Automata (CA) rule was implemented, namely, The Conway’s Game of Life. The Game of Life is a simulation which models the life cycle of bacteria using a 2D cell grid. Given an initial pattern, the simulation runs the birth and death of future generations of cells using a simple set of rules. CA is a model of a system of cell objects with the characteristics of living in a grid, having a single state, and being aware of the neighbors. For now just imagine having a grid being occupied by a single living cell, now at each iteration the following set of rules will be applied to the neighbors of a cell.\n A location that has zero or one neighbors will be empty in the next generation. If a cell was there, it dies. A location with two neighbors is stable. If it had a cell, it still contains a cell. If it was empty, it\u0026rsquo;s still empty. A location with three neighbors will contain a cell in the next generation. If it was unoccupied before, a new cell is born. If it currently contains a cell, the cell remains. A location with four or more neighbors will be empty in the next generation. If there was a cell in that location, it would die of overcrowding.    Neighbors of a cell (natureofcode.com)   And that’s it, the final result of this task shows how the implementation runs for Conway\u0026rsquo;s simulation and reveals the bacteria growing over time.\n  The Game of Life in execution   For the next milestone the tasks will be focused on improving the inner implementation of the cellular automata to make it more flexible to the distinct types of elements that the simulation will handle in the future (water). Also to implement the paint feature of solid elements in the canvas.\nMilestone 2 Improving the implementation So far the experience with Rust has been delightful but also a bit painful. There have been some concepts that needed to take time and practice, this is memory handling. Rust is not as permissive as other languages when it comes to memory errors. Usually all the work has to be done upfront. This means that all the handling of null variables, undefined states or memory corruption is checked at compile time. In general I’m getting used to this kind of programming because it helps a lot when dealing with huge amounts of instances that are supposed to be unpredictable and change over time. To this extent the code has been improved as I could. I’m not implementing my own cargo libraries yet but soon I will because code modularization is becoming a need.\nImplement the brush feature of solid cells For this part I decided to make some changes in the draw logic of the simulation. First, I got rid of the initial implementation which started a new simulation based on a file which described the initial state of the cell world. Now it starts with a blank canvas to start drawing. The drawing part is made of mouse clicking and dragging. Internally does not draw every single trace in which the mouse has been located. Instead is updated on a fixed interval and it draws a fixed size of pixels on the mouse location. Two sizes of brushes were implemented, a small one and a big.\n  Two sizes of brush   But as you may notice this is not being animated, because it should be like that. This brush is a solid element which has to collide with the water in the future. However I also wanted to have my early simulation working with this. So, how do I evolve the cells in the canvas taking into consideration the new constains? I decided to implement that task for the next milestone. For now I just got fun and implemented the drawing feature with different types of brushes ,or may say, different types of CAs (the most interesting ones). Also the animation can be paused to have the opportunity to draw and see how this evolves without any evolution and then simulate again. That’s it for now. I have been also implementing coloring and multithreading but it’s buggy for now.\n  CA shapes as brush type   Milestone 3 Implementation of fluid dynamics At first, before choosing this project idea in the first place I had a vague idea of how exactly I should implement the behaviour of water in a Cellular Automata fashion. Now, after devoting a decent amount of time doing research in the topic I’ve found different strategies to tackle the problem. I’ve implemented two of them.\n1-Dimensional CA This method is based on this post in which the simulation is made using a single dimension of cell interaction. Each cell in the vector has 3 values: elevation, water, volume and kinetic energy Then the simulation works with the values generated by those inputs: potential energy and directional pressures. In every iteration, each cell calculates the amount of pressure with their neighbors and the difference between them and itself. Later, each cell finds a value used to determine the amount of water going to the next direction. These values are added at the end of the iteration. The implementation required to tweak the initial implementation quite a lot since it required downgrade the dimensionality of the cell space. The result I got was not compelling, as it required more time to fix and debug. I decided to try another approach instead.\n  1D CA Fluid simulation   2-Dimensional CA Using two-dimensional space of cells to represent the world was the way to go. Each cell can contain different elements and interact with others. The algorithm is heavily inspired by the Chapter 2.6 of the book Game Programming Gems Series v. 3. The basic idea is to start by considering two or more water cells stacked vertically. When this occurs, then the cell of the bottoms starts to get more water level than the others. This way we avoid tracking expensive calculations of pressure to make water equalize. We only get the current value level of the water and move it upwards when is stacked up. The list of constants used for the simulation are: flow, mass, speed. The algorithm requires that if one cell contains 1.0 units of water, the cell below it should contain up to 1.02 units, the next one 1.04, and so on by a factor of 0.02. There is a special case to get the simulation run fluid. The bottom cell has to contain a proportionally smaller excess amount of water and compression than the others. The water movement is made by evolving the generation of cells of the mass matrix with the following rules:\n Get how much water the bottom cell should contain using the mass of the current cell and the cell below. If is below 0 (or the threshold), remove the corresponding amount from the current cell and add to the bottom cell Update the cell to the left. If it has less water, move over enough water to make both cells contain the same amount. Repeat the process for the right cell Same procedure as step 1 but with the upper cell.  It is required to keep track of the current water values on each generation to be updated in the next one The computation is a bit expensive but provides a better looking simulation.\n  2D CA Fluid simulation   The author of the book suggests many other elements (fire, heat, air) that can be implemented using the same approach. It might be interesting to do them but I’ll rather try to fix some quirkiness that the current implementation has.\nMilestone 4 The previous milestone had the problem of the water cells getting disappeared when they are simulated. To extend the goal for this milestone was to improve the initial implementation and find a solution for the problem. The first solution that came to my mind was the mass values of each cell when they communicate with the neighbors by applying the second rule. However this seemed to work because I had to run step by step and inspect at the values getting updated properly and they were doing correctly. Then after many iterations I discovered that the temporal array of cell values is cleared when performing a new iteration. This leads to empty values in the next generation of water cells by trying to modify its water mass with zero values. The solution then was to correctly handle both structures in order to prevent inconsistency. The simulation takes a constant of compression to keep the level of water on the same ground. This constant relies heavily on the maximum value of the mass units of water.\n  Simulation with basic coloring   As it can be seen, the water gets accumulated properly and follows the rules even when the ground behind it gets removed. However the color can be improved, since each cell stores the amount of water units this can be translated into a gradient of colors to get the aspect more realistic.The approach for this required to interpolate the range of mass values to the range of the blue color space. Finally in order to give the dark-purple aspect then the red value has to follow a slight similar pattern.\n  Simulation with improved coloring   Finally, having the simulation working properly the task now was to improve the performance. The basic implementation ran seamlessly up to 300x300 of buffer pixel resolution. Beyond that point it gets slow and breaks the real time experience. For that extent many options were taken into consideration: parallel processing, quadtree, hash maps, shader rendering, efficient array utilization, and so on. The first chosen option was to use shaders and rely on the GPU to run efficiently the cells of the simulation. However, since this implementation was relying heavily on a library that does not support to run shaders explicitly, I have to move the graphic implementation to a one that both supports: handy buffer management and shader rendering. Mini_gl_fb was the answer. This is a fork of the original library I’m using but it supports shader compilation and custom event handling. The code became larger as this library was ported and the result improved a bit. This was because all the calculations were still made on the CPU side. A second option was considered, parallel processing. The first and simple idea to implement was by dividing the canvas into equal region blocks and designating each region to a separate process. For example if the grid size is 120 x 300 then the simulation will subdivide into 2 rows and 3 columns of smaller grids.\n  Grid size     Subdivisions of the grid   The idea is to perform local computation on each process, then communicate them by having process exchange of values who have cells in common. Once the communication is done then perform the next cell generation. The implementation can be more efficient by avoiding the loop over each cell in the grid and sending the values individually to send them all at once but this is a matter for future exploration.\nRegarding the feature of rotating the canvas, although I didn’t manage to research it further, I read about some caveats to implement it. I found a thesis about it called Rotations in 2D and 3D discrete spaces by Yohan Thibault and he explains the constraints and challenges that it might face the process of rotating. The loss of precision, lack of neighbor connectivity and artifact appearance are between the common ones. However I’m planning to give it a try and depending on the complexity I may constrain the rotation with 90 degrees steps, and not the total degree freedom as was originally proposed.\nMilestone 5 For this milestone, the goal was to implement the proposed feature of canvas rotation. Initially the conceived idea was to give the user the freedom to rotate the canvas in any angle. As the research went through, the solution seemed not quite trivial and, as mentioned before, a thesis I found detailed the major challenges to tackle this task. In a nutshell, their main problem with making rotations in 2D discrete spaces is the loss of definition as it is shown in the figure below. This happens because the grid to be rotated does not follow the rules of an Euclidean space. To this end the author also proposes a solution using hingle angles which is mutating the matrix with a function of error minimization.\n  Cumulative pixel lost on canvas rotation   After trying to make a naive implementation on the current simulation, I noticed that there is not so much value to give this feature if the resolution is so low. And considering that the performance is not quite good on high resolution at the moment, an implementation of the rotation was made following the constraints of 90 degrees of freedom. This means that the user can rotate the canvas either clockwise and counterclockwise by one complete rotation at time. To make this possible, a series of transformations over the matrix buffer was required. Rotation by 90 degrees requires to transpose and then reverse each row. Rotation by -90 degrees requires to transpose and then reverse each column.\n  90 degrees matrix rotation   As a general rule for keeping the properties of the water simulation, only static objects are the ones who can rotate, meaning only blocks who do not interact with gravity. The GLSL code is shown bellow.\nvoid rotateCanvas(ivec2 xy_curr, Cell curr) { if (xy_curr.x \u0026lt; u_resolution.x / 2) { if (xy_curr.y \u0026lt; u_resolution.y - xy_curr.x - 1) { if (curr_gen[toIndex(ivec2(u_resolution.y - 1 - xy_curr.y, xy_curr.x))].type != CELL_WATER) { next_gen[toIndex(xy_curr)] = curr_gen[toIndex(ivec2(u_resolution.y - 1 - xy_curr.y, xy_curr.x))]; } if (curr_gen[toIndex(ivec2(u_resolution.y - 1 - xy_curr.x, u_resolution.y - 1 - xy_curr.y))].type != CELL_WATER) { next_gen[toIndex(ivec2(u_resolution.y - 1 - xy_curr.y, xy_curr.x))] = curr_gen[toIndex(ivec2(u_resolution.y - 1 - xy_curr.x, u_resolution.y - 1 - xy_curr.y))]; } if (curr_gen[toIndex(ivec2(xy_curr.y, u_resolution.y - 1 - xy_curr.x))].type != CELL_WATER) { next_gen[toIndex(ivec2(u_resolution.y - 1 - xy_curr.x, u_resolution.y - 1 - xy_curr.y))] = curr_gen[toIndex(ivec2(xy_curr.y, u_resolution.y - 1 - xy_curr.x))]; } if (curr.type != CELL_WATER) { next_gen[toIndex(ivec2(xy_curr.y, u_resolution.y - 1 - xy_curr.x))] = curr; } } } }   Constrained canvas rotation   As a bonus feature, I wanted to make a procedural map generation for the water to flow as a quick to-go. For this, a cave map generator based on cellular automata was implemented. This generator returns us a two-dimensional array of blocks, each of which is either solid or empty. So in a sense it resembles a scene of games like dungeon-crawlers with random levels for strategy games and simulation.\n  Iterative process of CA cave generation   To this end, the procedure is iterative. First we start out by randomly setting each cell to either block or empty. Each cell will have the same random chance of being made alive, 45% in this case. Then after counting the neighbors of each cell we’re going to proceed with the rules of the CA. We have two special variables, one for birthing dead cells (birthLimit), and one for killing live cells (deathLimit). If living cells are surrounded by less than deathLimit cells they die, and if dead cells are near at least birthLimit cells they become alive. Then it’s a matter of try how many iterations this cave generation will take place. The more iterations, the more smoothness can be achieved.\nfn do_cave_generation_step( \u0026amp;self, old_map: [[bool; WIDTH]; HEIGHT]) -\u0026gt; [[bool; WIDTH]; HEIGHT] { let mut new_map = [[false; WIDTH]; HEIGHT]; let death_limit = 3; let birth_limit = 4; for i in 0..WIDTH { for j in 0..HEIGHT { let nbs = self.count_neighbours(old_map, i, j); if old_map[i][j] { if nbs \u0026lt; death_limit { new_map[i][j] = false; } else { new_map[i][j] = true; } } else { if nbs \u0026gt; birth_limit { new_map[i][j] = true; } else { new_map[i][j] = false; } } } } new_map }   Procedural block generation taking interaction with the fluids   Now speaking of performance improvements. I have been implementing two methods concisely. Parallel processing and compute shaders. I didn’t achieve any positive results from the first method. It required further tweaks and definitely the time to be spended increased a lot. For that reason I decided to give a try to compute shaders. At first I tried to implement the basic Game of Life and the results were quite compelling. The port was relatively easy to do. However when I faced the task of porting the fluid simulation in the shader’s paradigm I encountered many challenges on the way that at the end resulted in me having to look more in depth. Mainly, the problem was that I needed to set up a pipeline of computed processes. The first one for updating the mass value of the cells, and the second pipeline for applying the values of the first buffer into the second by updating the corresponding element type. Since it was the first time doing this it consumed a lot of time and I decided to postpone it for the next and last milestone along with the performance metrics using the criterion library with the test cases that are going to be evaluated.\nMilestone 6 The last milestone of this project was devoted to implementing a bonus element; but first and foremost, to measure and improve the performance of the simulation. To this extent, a GPU version of the simulation was created from scratch, using OpenGL, GLSL and Rust because trying to port the current code to work with shaders was more of a hassle since the library didn’t allow for setup a more complex pipeline rendering. This process involved porting the main computation of the cells into computer shaders.\nBenchmark The benchmarking was done in one computer with three different map setups and two different approaches of rendering: GPU and CPU. The technical specification of the machine is as follows: CPU: Intel i7-7500U @ 3.500GHz 4 cores, RAM: 8GB, GPU: Intel HD Graphics 620. The testbed consisted of three scenarios with different grid sizes to perform the simulation: 250, 500 and 1300 respectively. On each of those, the captured metrics were FPS and time to compute a single simulation step. The resulting average FPS gathered during the evaluation of the three scenarios are shown in the figure below.\n  Plot of the time to compute each generation     Plot of FPS mean during the simulation   In addition to the CPU version of the CA, the GPU version underlines the effect of parallelization on computation time. It is not surprising that the simulation runs faster on the GPU, but it is still interesting to see the difference in performance. In the figure, with a small grid size the performance is very similar, but as long as the grid size increases we observe that the GPU version is not only much faster than the CPU version but it also scales better. The computation time on the GPU is so small that it scarcely affects the framerate. With this in mind we can say that the computation costs depend on the size of the CA. The size, in turn, depends on the number of cells as well as the number of different elements and is limited by RAM (CPU) and VRAM (GPU).\nBonus element Since the GLSL version of the code gave room for more graphics experimentation, first the styling was improved, a brush cursor with custom size was introduced, and the new element was well. The extra element is the “acid” or “virus” which behaves by destructing every other “ground” cell that is on its way. It reacts with gravity and it does not last for long.\n  Revamped GPU based simulation   As a conclusion of this milestone I can say that keeping a good performance with an increased grid size was at first a challenge because there were many techniques to approach for dealing with the optimization, however the simplest and cleanest in my opinion was the use of the GPU for computing the simulation, this allowed not only write a cleaner code but also to take advantage and apply more complex shading.\nConclusion This kind of simulation based on cellular automata, and grid evolution requires a high iteration rate on each frame. By increasing the grid size to make the simulation more realistic comes at the expense of more CPU cycles per frame, making the experience unpleasant. A GPU implementation of the simulation was done to aliviate this. The whole code of the rules was ported to compute shaders and the benchmark shows a dominant increase in performance. I think that was one of the rewarding experiences of this project. I want to thank my classmates and Raimond for the advice and support.\nLinks  Source code 2D Liquid Simulator With Cellular Automaton in Unity Journey into rust #1: Conway\u0026rsquo;s Game Making Sandspiel When Parallel: Pull, Don\u0026rsquo;t Push The Book of Shaders: 1, 2, 3  ","permalink":"https://0x7b1.github.io/projects/ca-fluid/","summary":"Implementation of fluid/liquid dynamics using Cellular Automata in Rust with CPU/GPU benchmarking.","title":"Cellular Automata Fluid Simulation"},{"content":"In this project the goal was to develop a streaming system which can do real-time analysis of temperature data of multiple room sensors. Two types of analysis were done: compute the relation between temperatures from two different sources, and trigger alerts based on real time conditions. Large number of sensors should be able to send data in real-time, and should be able to receive the feedback and result in a comprehensible time in a dashboard. Keeping the requirements in mind we used Apache Kafka and Flink to develop the system. In this setting, a custom application runs Kafka producer client, which is sending data in real-time with randomed sensor data. Flink streaming is used to process data coming from Kafka on a single topic to analyze the temperature measurements. We have developed the system as a visual and interactive dashboard tool.\nHow to execute Setup the environment The project has been packed to easily work with docker containers, that\u0026rsquo;s why the only requirement to have is Docker. Once we have Docker up and running:\n$ git clone https://github.com/0x7b1/heatmap-alert-app.git Start the Kafka cluster Navigate to the folder event-source, and build and run the Kafka cluster on background.\n$ cd event-source $ docker-compose up -p kafka-cluster -d --build Start the Flink cluster Go back again to the root folder and build and run the Flink cluster on background.\n$ cd .. $ docker-compose up -p flink-cluster -d --build It might take some time to have both clusters built and running, but once we have it we can access to the following set of available ports.\n 8080: Grafana dashboard 8081: Flink Web dashboard 9000: Kafka Manager 9090: Prometheus Dashboard  System Design The system is designed using Kafka and Flink. Kafka is used to produce and receive temperature sensor measurements. Flink is used to receive the temperature records from a single Kafka topic. Flink is also used to calculate the relation between different temperature sources over the windows of data streams, as well as generating alerts based on certain threshold conditions.\nApache Kafka Apache Kafka is a distributed, high-throughput message queuing system designed for making streaming data available to multiple data consumers. Kafka makes the streaming data persistent by storing incoming messages using a log data structure. This allows multiple stream consumers to read the stream at different positions (offsets) and different speeds, and also read messages from a certain point of time. The most important abstraction in kafka is the topic. A topic is a category name to which records are stored and published. Topics are divided into partitions. Partitions allow topics to be parallelized by splitting the data into a particular topic across different physical nodes. Services that put data into a topic are producers, and consumers are those read from a topic. In our current system abstraction, sensor data is published to a particular topic designated for the stream. This record corresponds to a particular room identifier within a defined incoming source, meaning “out” or “in”. This topic matches with a set of partitions that is meant to be used by each particular room. This capability is used to guarantee the ordeness or message processing.\nApache Flink Apache Flink is a scalable and fault tolerant data stream processing framework that runs on self-contained streaming computations that can be deployed standalone or using a resource manager. Flink consumes streams and produces data into streams. Flink is a true streaming engine, treating batch as a special case with bounded data. This feature is quite the opposite compared to other streaming frameworks like Spark Streaming or Kafka Streaming. Flink is commonly used with Kafka as the underlying storage layer, but is independent of it. Flink’s rich API allows to model a problem using data transformations following the paradigm of data flow. It also lets us use advanced features such as watermarks and timestamps, and Complex Event Processing (CEP). For our system, we make intensive use of data transformation functions, event windowing and also pattern matching.\nSystem Architecture The system architecture is shown above. It all starts by a single Kafka program which acts as a data producer in the Kafka model. The Kafka instance runs along with Zookeeper, as the service for coordination and configuration within the cluster. The Kafka manager helps by providing a web interface to see and manage the current state of Kafka and Zookeeper configurations. The produced data are temperature readings, simulating to those who come from real sensors. This data is published into a specific topic for further processing. In this system the assumption is that any sensor can join or leave the streaming pipeline and be able to persist data at any point in time. Kafka takes care of the scalability of the system as the number of incoming temperature records grow or shrink. It also serves as a data retention, storage and forwarding interface. Ideally kafka treats every message from a specific room into a corresponding partition, but it may be also the case that there are less partitions than topics. For such a case a round robin technique is used.\nWhen data arrives to Flink, two tasks are going to process them in the data flow pipeline, “Temperature Relation Processing”, and “Temperature Alerting” which are going to be detailed next. Later, the results are sinked into InfluxDB as a time-series event. This output can also be sent back to Kafka for further processing but we just want to deliver the results as early as we have. Prometheus will constantly monitor Flink to retrieve and store system metrics used later for the evaluation. On the last frontier we have Grafana that pulls data from Prometheus and InfluxDB every interval of time to show in an interactive dashboard the results of the data pipeline scheme.\nData Pipeline The algorithms used in the data transformation along the flow of the graph are discussed in more detail in the next section. When new data arrives to Flink from Kafka, the consumer will split into two main branches making reference to the incoming source, either from “IN” or “OUT”. Each task will be processed by a single Flink Task Manager. Also we adjust the level of parallelism to match the Kafka partitions into Flink Slots, in that way we guarantee correct order of arrival of messages.\nThe figure below shows the complete data pipeline including the processing of Temperature Alerts. At first glance it seems that the splitting of data is further processing and filtered out. At the end data joins again to output to a single sink in each case. For the temperature relation part, data is sent to two sinks, each corresponding to a specific source. An extra sink is used to output to the computed relation. The alerting does not sink to any particular source, but it keeps an internal counter meant to be fetched by Prometheus.\nExperiments and results The prototype system was implemented in Java. The program that simulates the sensor temperatures is a Java application that resides in the same cluster as Kafka. The implementation of the two temperature tasks are Java applications which are sent to a separate cluster for Flink. The clusters do both real-time analysis and produce the results to be shown in a custom made Grafana dashboard.\nThe first experiment was made with 5 room sensors under a load of emission of 5 random temperatures per second. The figure shows:\n The results in two plots: A heatmap that visually shows the relation between the temperatures IN and OUT, being the darker one considered as the OUT greater than IN. The set of gauges per room, which show three different colors based on three thresholds (10, 20, 30) When two consecutive warnings are emitted then new alerts are shown. The graphs related to the performance evaluation. We are mainly plotting the memory usage of the Flink Job Manager and Task Manager. With this we want to know the task needs memory in order to get the results. We noticed that Flink does a great balance among different Task Managers when more parallelism is enabled. Plot of the average latency for the Flink pipeline in each pass under the defined load. We see that it does not fluctuate violently, keeping in the range of 300 - 350 ms.  Further improvement can be done from this point. The CEP patterns can be fine tuned to detect trends in temperature. Also we could potentially forecast new scenarios based on predefined window times. Also, it would come in handy to have a better random producer of data measurements, for example by taking a real data set and feeding it into the current streaming system.\nLinks  Source code  ","permalink":"https://0x7b1.github.io/projects/heatmap-alert/","summary":"A tool for automating the structure, documentation and tracking of a new software project.","title":"Heatmap Alert"},{"content":"This project implements the mechanics of Earthbending in VR using Unreal Engine and C++. In the essence the Earthbending is a way to interact with the ground. Our goal is to design an algorithm and figure out controls of earthbending magic. In the most abstract blueprint the task consists of procedural mesh editing and VR technology integration.\nGame style The first idea that came was the game style. It had to be presented in a way that does not require a powerful machine to run complex geometries and fancy effects because we have the limitations of the Oculus Quest device. For this we came with the idea of low poly style. This implied to have meshes with minimum geometry complexity and straightforward texturing. Besides, low poly should allow us to design the level and make changes quicker. Also low poly looks great with proper shading as these examples.\n  Game design The game mechanics played a huge role on the proyect. As initially said, we had the idea of providing the experience of playing with the earthbending effect. We thought that would be great to make it on a VR experience. Then, shaping it into a more game looking experience we designed a game map that compasses the levels, enemies and goals that a proper game should have. This is how the initial game map idea looked like\n  Level concept   This is basically a platform game that requires to the user to climb level by level to reach the highest point and win. Each level is composed by one or many islands in which the enemies are spawned to interrupt and attack the player. Then after getting some great 3d assets from here and here the task was now to build the actual game map. After many hours of level design we got this.\n  Side view (wireframe)     Side view (preview)     Perspective view   Game characters We also needed to get or build actors in the game. For this purpose we got assets to make the game more playable.\nEnemies We have enemies that have the role of blocking the path of the main player character. Each enemy has the same defined behavior: patrol the island, look for the player, attack and die. This is the list of complete enemy actors.\n  Models by @quaternius   Main Game Player The player is the main actor. It is spawned on the first level and hast the goal to reach the highest level. The player has defined interactions that will be described below. Worth to note that we were developing for two types of view player. As First Person Shooter and as VR Person. The first one mainly for developing purposes and the other for the real game interaction. This came with tradeofs to put more effort into one and to take the time to implement more logic into the other. As this is a lesson learned, it will be detailed later.\n  VR Camera Actor   VR integration Oculus Quest was the VR headset of choice because it offers a standalone experience with no extra wires while you are playing. This comes with a cost - a running power. For that extent the natural way to proceed was first to test some applications and some demos testing out the complexity and capacity of the device because in every VR immersion we don\u0026rsquo;t want to experience lag in the game. The first step was to choose of the right tool for developing the game. The obvious options that comes to one\u0026rsquo;s mind is between Unity and Unreal Engine. The task was to explore which of them provides less friction in the integration of VR and the game. Both tools provide good documentation and plugins to work with VR right away but we ultimately pick Unreal Engine because of the ease of prototyping cases, the graphics, and the fact that it works with C++. Here is a basic integration in both softwares showing the mapping between the character\u0026rsquo;s hand and the controls.\nThen, the task was concerned to bring some interaction into the scene. At first, a quick placement of objects with integrated physics was made to be sure they act fluidly once deployed into the headset. It turned out that when the build process started it took quite a long time to get finally into the Quest (like 1hr) because it was required to compile the shaders, build the lights, build the scene, build everything in an android-like package and then it was ready to be tried out. Some time was spent trying to test corner-cases when potentially the physics could break but none were found. Next build and deployment time was faster. Adding more polygons and more complex objects seemed to be ok with this experiment.\nThe next steps were to make the graphics scene of the game, integrate the control buttons to interact with the polygons and area of the floor to have the Earthbending effect. After that the path was to make this more immersive with grain-defined interactions and gameplay. More of the code and blueprints are at the repository.\nGameplay Teleportation Each island can be navigable by the player and the enemy. For doing this, the user has to press and maintain the A button of the VR Control to point into the desired destination and release the button in order to teleport. This was modified to work only on the right hand.\n  Climbing To step up into the next level island the player has to climb the ladder that is located near the edge of the last island. The user has to reach the highest point and realease the grabbing and landing into the next level in which again should have to reach the next ladder. For making this feature possible, a complete understanding of the VR Motion Controller implementation was required (this was also useful later to implement the bending). The climbing was implemented to work on both hands by pressing the backside trigger of the control.\n  Patrolling The enemy is an implemented NPC with basic AI rules. On each level has predefined points what will serve to go from one to another to mimic the behaviour of defending a zone. When the enemy is being distracted or it senses the Player within the defined area, it chases for him.\n  Attacking The player has te ability to attack by bending (defined bellow) and shooting (from FPS view). The enemy attacks by getting close to the player and hitting straight to to the player position. When the player gets far from the enemy, this has the predefined behaviour of following to chase and keeping attack if the player is still within the range.\n  Bending The main player has te ability to attack by bending the terrain of the island in which is currently standing. It makes damage to the enemy when the enemy is in the area of attack.\n  Conclusions We faced many challenges along this journey. We first had to get familiar with the engine and the blueprint terminology. Then the implementation was something that iterated a lot. First with the initial idea of terrain blending by modifying vertex on runtime was not a trivial task. Then we implemented using material behaviour and we got an appealing result. Also implementing for VR was challenging from the first moment. The controlls, the mechanics and optimizations were subjects to be concerned on each stage.\n  Links  Source code  ","permalink":"https://0x7b1.github.io/projects/earthbending/","summary":"A game that explores the mechanics of VR by implementing a clever algorithm for procedural earhbending.","title":"Procedural Earthbending"},{"content":"   Traffic CO2 emission is a constant problem that have an impact into the environment. There are proposed solutions that improve the traffic analysis and operation performance to reduce traffic congestion and increase overall safety. However, these solutions have become increasingly complex due to the inclusion of multiple tools for treating the traffic management at a realistic manner. Emission models, that represent a separate complex task on its own, have been introduced in these systems making them even more difficult to scale. ITS needs to have easy and effective mechanisms for interacting with the existing vehicle infrastructure and be able to accomplish one task right. This project explores the implementation of a system that focuses on the gathering and calculation of CO2 emissions of urban vehicle traffic. The system presented is a cloud-based infrastructure that has been tested using different simulation scenarios with the objective of provide realistic urban traffic behaviour. The work also looks at the value of having a wide range of perspectives to evaluate transport pollution based on the existing retrieved information.\nSimulator The purpose of this simulator is to calculate the emissions caused by a connected urban car traffic in real-time though a web system interface.\n  Components of the system   System Components The presented system relies on connected devices representing the vehicles.\nDevices The system will collect data from a set of connected devices. These devices may not be able to report successfully at the desired rate due to intermittent connectivity. Additionally, this reported data contains a certain rate of error.\nStorage Storage Data collected by devices store the current state of the device entities. They are processed before being stored. The data is also stored for current and future analysis.\nController The system operates with a controller that acts on the input from devices in real-time. Then the data is stored and preprocessed into the storage component. The controller is also used to retrieve data for the Web client application.\nMonitoring The processed data is displayed in a web component that contains tools for visualization. This operates by collecting up and grouping the data of the storage component.\nImplementation   System components of the simulated implementation   The back-end application acts as the message gateway by providing communication over HTTP and potentially over the MQTT protocol. Then, a time-series database, namely InfluxDB, is used to storage received messages as a real time telemetry of vehicles. Data from traffic is published to a common data channel into a stream end-point. Vehicles emit messages to the controller containing the entity id, speed, latitude, longitude, and acceleration. The messages are processed by a function that calculates the emission based on these attributes. It is then possible for the emission to be calculated concurrently. This ensures multiple evaluations while ensuring processing in real time.\nCO2 Simulation An exact and efficient calculation of CO2 emissions depend on many different factors, such as the model of the vehicle, year of fabrication, fuel, engine specifics and other technological features. A good overview of the standard emission methods for vehicle emissions can be seen here. The proposed system uses a microscopic approach of simulation, in which each vehicle hast its own representation. Individual vehicle characteristics may include factors such as vehicle type, weight, speed, acceleration, engine performance, between others. So the level of detail is even higher at sub-microscopic models. They can include vehicle shifting, steering, fuel consumption and so on. Regarding of any type of model, emission can be attained into each flow model. For this reason, the microscopic modeling is used because it helps to calculate the emission of CO2 of passenger vehicles.\nThe method of this paper uses the following definition for the estimated emissions to approximate the generated CO2 emissions as defined by:\n$$ P = c_0 + c_1va + c_2va^2 + c_3v + c_4v^2 + c_5v^3 $$\nwhere \\(P\\) is the emission frequency (in miligrams persecond), c are the emission constants, v is the vehicle’s speed in m/s, and a is the acceleration in \\(m/s^2\\). For this experiment we are going to use the constants \\(9449, 938.4, 0.0, −467.1, 28.26, 0.0\\). A heat-map generally visualizes data intensity at geographic points. As it is shown in the next figure we can navigate and zoom through the polluted areas.\n  Web application in live mode showing the trace of the vehicles as they come     On history mode the option for filtering and showing the layers is available   Evaluation We need a framework to run the simulated agents into a network to make realistic trips that also could provide a test environment. For this, we need to define a network representation of a road network. Using SUMO to make a real-world scenario from scratch is time consuming as one needs to define and constantly tweak every single detail of the network road. We could use the integrated SUMO tools that speed up this process, such as defining a network based on a realistic map from OpenStreetMap.\nWith this described setup, the simulation in ready to run. First the baseline scenario is executed, then it is followed by the Monaco scenario. The simulated data gives a good insight on how this could operate in a real life. Running the scenarios for few moments one can observe the large number of entries and hits in the database. This became a problem when a query was needed to process to keep flexibility. Fortunately InfluxDB allows to group and aggregate in order gain some performance.\n  Baseline map on a small area in the city of Tartu     Monaco SUMO Traffic (MoST)   An experiment ran for 50 minutes. On both cases we can see the emitted CO2 in the heat-map layer. This is modified over time to fit the real-time behavior. As we scale the view with zoom in and out the aggregation is being modified.\nResults Based on the experiment described on the previous sections, this section discusses the obtained results, the benefits of this approach, and its limitations. Testing the first scenario revealed problems on the web application that were not addressed while initially was being developed. This needed improvement in the code for displaying correctly the emissions over time. Another improvement was the code for the calculation of CO2, as the initial implementation of emission did not yield appealing results. The code for the query of the database was also a thing to consider in this regard. In parallel, debugging and running the scenarios on the SUMO graphical interface was crucial to keep the constant feedback.\n  Plot of speed on the Tartu scenario   In terms of CO2 emission calculation, it was compared with the SUMO built-in emission model HBEFA v.2.1 which considers parameters of vehicle such velocity and acceleration. In order to evaluate both under same conditions, the test was run on the same scenarios. A quantitative estimation is used to compare both models. According to the figure 11 the SUMO model generates 5643334 mg of CO2 emission, while the method of this paper gets 5643443 mg. On the second scenario the SUMO model generates 8554854mg of CO2 emission, while the method of this paper gets 7454854 mg.\n  SUMO CO2 values vs own implementation shows a mean error or 15% on MoST scenario   It is always discussed in the ITS traffic literature that electric cars emit less emission than old traditional cars. This is a consideration and improvement that could be added to a new version of the system. We could also add more pollutants based on vehicle and environment parameters. Also a point to consider is the realistic representation of a city because if we were to run on it we could make better decisions in terms of ITS policies. Note, that this system was only concerned with CO2 emissions and its applicability to the existing devices (i.e. mobile phones) into a visualization tool taking into consideration its reliability through a distributed approach. The ultimate goal of this kind of simulations is to make effect on improving the environment quality.\nConclusions With an increasing number of connected vehicles on the road, traffic congestion has become a daily problem affecting several aspects of modern society. One of the problems of traffic that concerns more to the environment is the pollution through the CO2 emission. We also have noticed that nowadays we have an scenario of vehicle drivers somehow connected to a cloud service, this can be leveraged to use them in order to get estimation of CO2 emission into the current state of traffic. A system was proposed to fulfill this requirement. It was discussed the necessity for building a solution and why it should be developed in a distributed way. Hopefully the visualization tool provides an insight in the decision making of ITS policies.\nLinks  Source code  ","permalink":"https://0x7b1.github.io/projects/co2-emulator/","summary":"Given a simulated vahicle traffic on different cities, this app allows you to visualize the levels of CO2 emmited.and configuration files.","title":"CO2 Emission Simulator"},{"content":"OSBox automatically synchronizes, manages dependencies and configuration files in any operating system. The prototype can be tried here.\n  Concept of OSBox in action   NB. This is still an ongoing project\nLinks  Source code  ","permalink":"https://0x7b1.github.io/projects/osbox/","summary":"OSBox automatically synchronizes, manages dependencies and configuration files in any operating system.","title":"OSBox"},{"content":"In the journey of learning React and Javascript in depth I took the opportunity to collaborate in the early development of the new React-based admin of Wordpress.\n  Links  Source code  ","permalink":"https://0x7b1.github.io/projects/wpcalypso/","summary":"Contribution to the JavaScript and API powered WordPress.com","title":"WordPress Calypso"},{"content":"  A shameful ripoff of the Rust logo   Genesis is a boilerplate project builder. It provides all necessary project structure for managing and start up a new project. The application can be used here.\nIntegrations  Trello Gitlab Google Drive Gitlab   Note: This is an internal project for a company. The documentation, and related information remains closed.\n Links  Source code  ","permalink":"https://0x7b1.github.io/projects/genesis/","summary":"A tool for automating the structure, documentation and tracking of a new software project.","title":"Genesis"},{"content":"As of 31th of December 2020, this is how my system looks like.\n-` jc@0x7b1 .o+` -------- `ooo/ OS: Arch Linux x86_64 `+oooo: Host: 20HQS0DN00 ThinkPad X1 Carbon 5th `+oooooo: Kernel: 5.9.14-arch1-1 -+oooooo+: Uptime: 3 hours, 44 mins `/:-:++oooo+: Packages: 1114 (pacman) `/++++/+++++++: Shell: zsh 5.8 `/++++++++++++++: Resolution: 1920x1080 `/+++ooooooooooooo/` WM: i3 ./ooosssso++osssssso+` Theme: Flat-Remix-GTK-Yellow-Darkest-Solid-NoBorder [GTK2/3] .oossssso-````/ossssss+` Icons: Flat-Remix-Yellow-Dark [GTK2/3] -osssssso. :ssssssso. Terminal: alacritty :osssssss/ osssso+++. Terminal Font: Cozette /ossssssss/ +ssssooo/- CPU: Intel i7-7500U (4) @ 3.500GHz `/ossssso+/:- -:/+osssso+- GPU: Intel HD Graphics 620 `+sso+:-` `.-/+oso: Memory: 3625MiB / 7720MiB `++:. `-/+/ .` `/ Favorite tools  Arch Linux Rust Go Dropbox Paper Jetbrains GoLand Jetbrains IntelliJ Toggl  Links  Source code  ","permalink":"https://0x7b1.github.io/projects/dotfiles/","summary":"Actively maintained personal system configurations and automation scripts.","title":"Dotfiles"},{"content":"","permalink":"https://0x7b1.github.io/about/","summary":"","title":"About"},{"content":"","permalink":"https://0x7b1.github.io/blog/","summary":"blog","title":"blog"},{"content":"","permalink":"https://0x7b1.github.io/projects/","summary":"projects","title":"Projects"}]