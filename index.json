[{"content":"Links  Source code  ","permalink":"https://0x7b1.github.io/projects/spark-rdf/","summary":"Systematic comparison of RDF relational schemas using Apache Spark.","title":"SparkSQL RDF Benchmarking"},{"content":"Links  Source code  ","permalink":"https://0x7b1.github.io/projects/mit-6.824-project/","summary":"Read academic papers of distributed and data systems and implementation of the Raft consensus algorithm in Go.","title":"MIT's 6.824 Distributed System Project course"},{"content":"Links  Source code  ","permalink":"https://0x7b1.github.io/projects/ca-fluid/","summary":"Implementation of fluid/liquid dynamics using Cellular Automata in Rust with CPU/GPU benchmarking.","title":"Cellular Automata Fluid Simulation"},{"content":"In this project the goal was to develop a streaming system which can do real-time analysis of temperature data of multiple room sensors. Two types of analysis were done: compute the relation between temperatures from two different sources, and trigger alerts based on real time conditions. Large number of sensors should be able to send data in real-time, and should be able to receive the feedback and result in a comprehensible time in a dashboard. Keeping the requirements in mind we used Apache Kafka and Flink to develop the system. In this setting, a custom application runs Kafka producer client, which is sending data in real-time with randomed sensor data. Flink streaming is used to process data coming from Kafka on a single topic to analyze the temperature measurements. We have developed the system as a visual and interactive dashboard tool.\nHow to execute Setup the environment The project has been packed to easily work with docker containers, that\u0026rsquo;s why the only requirement to have is Docker. Once we have Docker up and running:\n$ git clone https://github.com/0x7b1/heatmap-alert-app.git Start the Kafka cluster Navigate to the folder event-source, and build and run the Kafka cluster on background.\n$ cd event-source $ docker-compose up -p kafka-cluster -d --build Start the Flink cluster Go back again to the root folder and build and run the Flink cluster on background.\n$ cd .. $ docker-compose up -p flink-cluster -d --build It might take some time to have both clusters built and running, but once we have it we can access to the following set of available ports.\n 8080: Grafana dashboard 8081: Flink Web dashboard 9000: Kafka Manager 9090: Prometheus Dashboard  System Design The system is designed using Kafka and Flink. Kafka is used to produce and receive temperature sensor measurements. Flink is used to receive the temperature records from a single Kafka topic. Flink is also used to calculate the relation between different temperature sources over the windows of data streams, as well as generating alerts based on certain threshold conditions.\nApache Kafka Apache Kafka is a distributed, high-throughput message queuing system designed for making streaming data available to multiple data consumers. Kafka makes the streaming data persistent by storing incoming messages using a log data structure. This allows multiple stream consumers to read the stream at different positions (offsets) and different speeds, and also read messages from a certain point of time. The most important abstraction in kafka is the topic. A topic is a category name to which records are stored and published. Topics are divided into partitions. Partitions allow topics to be parallelized by splitting the data into a particular topic across different physical nodes. Services that put data into a topic are producers, and consumers are those read from a topic. In our current system abstraction, sensor data is published to a particular topic designated for the stream. This record corresponds to a particular room identifier within a defined incoming source, meaning “out” or “in”. This topic matches with a set of partitions that is meant to be used by each particular room. This capability is used to guarantee the ordeness or message processing.\nApache Flink Apache Flink is a scalable and fault tolerant data stream processing framework that runs on self-contained streaming computations that can be deployed standalone or using a resource manager. Flink consumes streams and produces data into streams. Flink is a true streaming engine, treating batch as a special case with bounded data. This feature is quite the opposite compared to other streaming frameworks like Spark Streaming or Kafka Streaming. Flink is commonly used with Kafka as the underlying storage layer, but is independent of it. Flink’s rich API allows to model a problem using data transformations following the paradigm of data flow. It also lets us use advanced features such as watermarks and timestamps, and Complex Event Processing (CEP). For our system, we make intensive use of data transformation functions, event windowing and also pattern matching.\nSystem Architecture The system architecture is shown above. It all starts by a single Kafka program which acts as a data producer in the Kafka model. The Kafka instance runs along with Zookeeper, as the service for coordination and configuration within the cluster. The Kafka manager helps by providing a web interface to see and manage the current state of Kafka and Zookeeper configurations. The produced data are temperature readings, simulating to those who come from real sensors. This data is published into a specific topic for further processing. In this system the assumption is that any sensor can join or leave the streaming pipeline and be able to persist data at any point in time. Kafka takes care of the scalability of the system as the number of incoming temperature records grow or shrink. It also serves as a data retention, storage and forwarding interface. Ideally kafka treats every message from a specific room into a corresponding partition, but it may be also the case that there are less partitions than topics. For such a case a round robin technique is used.\nWhen data arrives to Flink, two tasks are going to process them in the data flow pipeline, “Temperature Relation Processing”, and “Temperature Alerting” which are going to be detailed next. Later, the results are sinked into InfluxDB as a time-series event. This output can also be sent back to Kafka for further processing but we just want to deliver the results as early as we have. Prometheus will constantly monitor Flink to retrieve and store system metrics used later for the evaluation. On the last frontier we have Grafana that pulls data from Prometheus and InfluxDB every interval of time to show in an interactive dashboard the results of the data pipeline scheme.\nData Pipeline The algorithms used in the data transformation along the flow of the graph are discussed in more detail in the next section. When new data arrives to Flink from Kafka, the consumer will split into two main branches making reference to the incoming source, either from “IN” or “OUT”. Each task will be processed by a single Flink Task Manager. Also we adjust the level of parallelism to match the Kafka partitions into Flink Slots, in that way we guarantee correct order of arrival of messages.\nThe figure below shows the complete data pipeline including the processing of Temperature Alerts. At first glance it seems that the splitting of data is further processing and filtered out. At the end data joins again to output to a single sink in each case. For the temperature relation part, data is sent to two sinks, each corresponding to a specific source. An extra sink is used to output to the computed relation. The alerting does not sink to any particular source, but it keeps an internal counter meant to be fetched by Prometheus.\nExperiments and results The prototype system was implemented in Java. The program that simulates the sensor temperatures is a Java application that resides in the same cluster as Kafka. The implementation of the two temperature tasks are Java applications which are sent to a separate cluster for Flink. The clusters do both real-time analysis and produce the results to be shown in a custom made Grafana dashboard.\nThe first experiment was made with 5 room sensors under a load of emission of 5 random temperatures per second. The figure shows:\n The results in two plots: A heatmap that visually shows the relation between the temperatures IN and OUT, being the darker one considered as the OUT greater than IN. The set of gauges per room, which show three different colors based on three thresholds (10, 20, 30) When two consecutive warnings are emitted then new alerts are shown. The graphs related to the performance evaluation. We are mainly plotting the memory usage of the Flink Job Manager and Task Manager. With this we want to know the task needs memory in order to get the results. We noticed that Flink does a great balance among different Task Managers when more parallelism is enabled. Plot of the average latency for the Flink pipeline in each pass under the defined load. We see that it does not fluctuate violently, keeping in the range of 300 - 350 ms.  Further improvement can be done from this point. The CEP patterns can be fine tuned to detect trends in temperature. Also we could potentially forecast new scenarios based on predefined window times. Also, it would come in handy to have a better random producer of data measurements, for example by taking a real data set and feeding it into the current streaming system.\nLinks  Source code  ","permalink":"https://0x7b1.github.io/projects/heatmap-alert/","summary":"A tool for automating the structure, documentation and tracking of a new software project.","title":"Heatmap Alert"},{"content":"This project implements the mechanics of Earthbending in VR using Unreal Engine and C++. In the essence the Earthbending is a way to interact with the ground. Our goal is to design an algorithm and figure out controls of earthbending magic. In the most abstract blueprint the task consists of procedural mesh editing and VR technology integration.\nGame style The first idea that came was the game style. It had to be presented in a way that does not require a powerful machine to run complex geometries and fancy effects because we have the limitations of the Oculus Quest device. For this we came with the idea of low poly style. This implied to have meshes with minimum geometry complexity and straightforward texturing. Besides, low poly should allow us to design the level and make changes quicker. Also low poly looks great with proper shading as these examples.\nGame design The game mechanics played a huge role on the proyect. As initially said, we had the idea of providing the experience of playing with the earthbending effect. We thought that would be great to make it on a VR experience. Then, shaping it into a more game looking experience we designed a game map that compasses the levels, enemies and goals that a proper game should have. This is how the initial game map idea looked like\nAs it can be seen, is basically a platform game that requires to the user to climb level by level to reach the highest point and win. Each level is composed by one or many islands in which the enemies are spawned to interrupt and attack the player. Then after getting some great 3d assets from here and here the task was now to build the actual game map. After many hours of level design we got this.\nGame characters We also needed to get or build actors in the game. For this purpose we got assets to make the game more playable.\nEnemies We have enemies that have the role of blocking the path of the main player character. Each enemy has the same defined behavior: patrol the island, look for the player, attack and die. This is the list of complete enemy actors.\nMain Game Player The player is the main actor. It is spawned on the first level and hast the goal to reach the highest level. The player has defined interactions that will be described below. Worth to note that we were developing for two types of view player. As First Person Shooter and as VR Person. The first one mainly for developing purposes and the other for the real game interaction. This came with tradeofs to put more effort into one and to take the time to implement more logic into the other. As this is a lesson learned, it will be detailed later.\nVR integration Oculus Quest was the VR headset of choice because it offers a standalone experience with no extra wires while you are playing. This comes with a cost - a running power. For that extent the natural way to proceed was first to test some applications and some demos testing out the complexity and capacity of the device because in every VR immersion we don\u0026rsquo;t want to experience lag in the game. The first step was to choose of the right tool for developing the game. The obvious options that comes to one\u0026rsquo;s mind is between Unity and Unreal Engine. The task was to explore which of them provides less friction in the integration of VR and the game. Both tools provide good documentation and plugins to work with VR right away but we ultimately pick Unreal Engine because of the ease of prototyping cases, the graphics, and the fact that it works with C++. Here is a basic integration in both softwares showing the mapping between the character\u0026rsquo;s hand and the controls.\nThen, the task was concerned to bring some interaction into the scene. At first, a quick placement of objects with integrated physics was made to be sure they act fluidly once deployed into the headset. It turned out that when the build process started it took quite a long time to get finally into the Quest (like 1hr) because it was required to compile the shaders, build the lights, build the scene, build everything in an android-like package and then it was ready to be tried out. Some time was spent trying to test corner-cases when potentially the physics could break but none were found. Next build and deployment time was faster. Adding more polygons and more complex objects seemed to be ok with this experiment.\nThe next steps were to make the graphics scene of the game, integrate the control buttons to interact with the polygons and area of the floor to have the Earthbending effect. After that the path was to make this more immersive with grain-defined interactions and gameplay. More of the code and blueprints are at the repository.\nGameplay Teleportation Each island can be navigable by the player and the enemy. For doing this, the user has to press and maintain the A button of the VR Control to point into the desired destination and release the button in order to teleport. This was modified to work only on the right hand.\nClimbing To step up into the next level island the player has to climb the ladder that is located near the edge of the last island. The user has to reach the highest point and realease the grabbing and landing into the next level in which again should have to reach the next ladder. For making this feature possible, a complete understanding of the VR Motion Controller implementation was required (this was also useful later to implement the bending). The climbing was implemented to work on both hands by pressing the backside trigger of the control.\nPatrolling The enemy is an implemented NPC with basic AI rules. On each level has predefined points what will serve to go from one to another to mimic the behaviour of defending a zone. When the enemy is being distracted or it senses the Player within the defined area, it chases for him.\nAttacking The player has te ability to attack by bending (defined bellow) and shooting (from FPS view). The enemy attacks by getting close to the player and hitting straight to to the player position. When the player gets far from the enemy, this has the predefined behaviour of following to chase and keeping attack if the player is still within the range.\nBending The main player has te ability to attack by bending the terrain of the island in which is currently standing. It makes damage to the enemy when the enemy is in the area of attack.\nConclusions We faced many challenges along this journey. We first had to get familiar with the engine and the blueprint terminology. Then the implementation was something that iterated a lot. First with the initial idea of terrain blending by modifying vertex on runtime was not a trivial task. Then we implemented using material behaviour and we got an appealing result. Also implementing for VR was challenging from the first moment. The controlls, the mechanics and optimizations were subjects to be concerned on each stage.\nLinks  Source code  ","permalink":"https://0x7b1.github.io/projects/earthbending/","summary":"A game that explores the mechanics of VR by implementing a clever algorithm for procedural earhbending.","title":"Procedural Earthbending"},{"content":"Empty for now\n","permalink":"https://0x7b1.github.io/projects/co2-emulator/","summary":"Given a simulated vahicle traffic on different cities, this app allows you to visualize the levels of CO2 emmited.and configuration files.","title":"CO2 Emission Simulator"},{"content":"Empty for now\n","permalink":"https://0x7b1.github.io/projects/osbox/","summary":"OSBox is the solution for managing the software stack in any operating system. It automatically synchronizes, manages dependencies and configuration files.","title":"OSBox"},{"content":"Empty for now\n","permalink":"https://0x7b1.github.io/posts/2018-07-22-triangle/","summary":"Have you ever wonder why barycentric coordinates are important? Take a look to this interactive explanation.","title":"Interpolating in triange"},{"content":"Empty for now\n","permalink":"https://0x7b1.github.io/projects/wpcalypso/","summary":"Implementation, refactoring and fixing features of the new React-based admin of Wordpress.","title":"WordPress Calypso"},{"content":"Empty for now\n","permalink":"https://0x7b1.github.io/projects/genesis/","summary":"A tool for automating the structure, documentation and tracking of a new software project.","title":"Genesis"},{"content":"As of 31th of December 2020, this is how my system looks like.\n -` jc@0x7b1 .o+` -------- `ooo/ OS: Arch Linux x86_64 `+oooo: Host: 20HQS0DN00 ThinkPad X1 Carbon 5th `+oooooo: Kernel: 5.9.14-arch1-1 -+oooooo+: Uptime: 3 hours, 44 mins `/:-:++oooo+: Packages: 1114 (pacman) `/++++/+++++++: Shell: zsh 5.8 `/++++++++++++++: Resolution: 1920x1080 `/+++ooooooooooooo/` WM: i3 ./ooosssso++osssssso+` Theme: Flat-Remix-GTK-Yellow-Darkest-Solid-NoBorder [GTK2/3] .oossssso-````/ossssss+` Icons: Flat-Remix-Yellow-Dark [GTK2/3] -osssssso. :ssssssso. Terminal: alacritty :osssssss/ osssso+++. Terminal Font: Cozette /ossssssss/ +ssssooo/- CPU: Intel i7-7500U (4) @ 3.500GHz `/ossssso+/:- -:/+osssso+- GPU: Intel HD Graphics 620 `+sso+:-` `.-/+oso: Memory: 3625MiB / 7720MiB `++:. `-/+/ .` `/ Favorite tools  Arch Linux Rust Go Dropbox Paper Jetbrains GoLand Jetbrains IntelliJ Toggl  Links  Source code  ","permalink":"https://0x7b1.github.io/projects/dotfiles/","summary":"Actively maintained personal system configurations and automation scripts.","title":"Dotfiles"},{"content":"","permalink":"https://0x7b1.github.io/about/","summary":"","title":"About"},{"content":"","permalink":"https://0x7b1.github.io/blog/","summary":"blog","title":"blog"},{"content":"","permalink":"https://0x7b1.github.io/projects/","summary":"projects","title":"Projects"}]